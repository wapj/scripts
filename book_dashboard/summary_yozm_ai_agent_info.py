"""
ì˜¨ë¼ì¸ ì„œì  ë„ì„œ ë­í‚¹ ì •ë³´ ìŠ¤í¬ë˜í¼
êµë³´ë¬¸ê³ , YES24, ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ìˆœìœ„ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import json
import logging
import re
import time
from datetime import datetime
from typing import Any, Dict, Optional

import httpx
from bs4 import BeautifulSoup

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class BookRankingScraper:
    def __init__(self, debug=False):
        """
        ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”

        Args:
            debug (bool): ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        """
        self.debug = debug
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        self.client = httpx.Client(
            headers=self.headers, follow_redirects=True, timeout=30.0
        )
        self.results = {}

    def fetch_page(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = self.client.get(url)
            response.raise_for_status()
            # httpxëŠ” ìë™ìœ¼ë¡œ ì¸ì½”ë”©ì„ ê°ì§€í•˜ì§€ë§Œ, í•„ìš”ì‹œ ëª…ì‹œì  ì„¤ì •
            if response.encoding is None:
                response.encoding = "utf-8"
            return response.text
        except httpx.HTTPStatusError as e:
            logging.error(f"HTTP ì˜¤ë¥˜ ({url}): {e.response.status_code}")
            return None
        except Exception as e:
            logging.error(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
            return None

    def scrape_kyobobook(self, url: str) -> Dict[str, Any]:
        """
        êµë³´ë¬¸ê³ ì—ì„œ ì£¼ê°„ë² ìŠ¤íŠ¸ ìˆœìœ„ ì¶”ì¶œ
        - êµ­ë‚´ ë„ì„œ ìˆœìœ„
        - ì»´í“¨í„°/IT ìˆœìœ„
        """
        logging.info("êµë³´ë¬¸ê³  ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        kyobo_data = {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "domestic_rank": None,
            "it_rank": None,
            "error": None,
        }

        try:
            html = self.fetch_page(url)
            if not html:
                kyobo_data["error"] = "í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return kyobo_data

            soup = BeautifulSoup(html, "html.parser")
            page_text = soup.get_text()

            # ì£¼ê°„ë² ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ - ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
            # êµ­ë‚´ë„ì„œ ìˆœìœ„ ì°¾ê¸°
            domestic_patterns = [
                r"êµ­ë‚´ë„ì„œ\s*(\d+)ìœ„",
                r"êµ­ë‚´ë„ì„œ\s*ì£¼ê°„ë² ìŠ¤íŠ¸\s*(\d+)ìœ„",
                r"êµ­ë‚´\s*ë„ì„œ\s*(\d+)ìœ„",
                r"ì¢…í•©\s*(\d+)ìœ„",
                r"ì¢…í•©ë² ìŠ¤íŠ¸\s*(\d+)ìœ„",
                r"ì£¼ê°„ë² ìŠ¤íŠ¸\s*êµ­ë‚´ë„ì„œ\s*(\d+)ìœ„",
            ]

            for pattern in domestic_patterns:
                match = re.search(pattern, page_text)
                if match:
                    kyobo_data["domestic_rank"] = int(match.group(1))
                    break

            # ì»´í“¨í„°/IT ìˆœìœ„ ì°¾ê¸°
            it_patterns = [
                r"ì»´í“¨í„°/IT\s*(\d+)ìœ„",
                r"ì»´í“¨í„°\s*/\s*IT\s*(\d+)ìœ„",
                r"IT/ì»´í“¨í„°\s*(\d+)ìœ„",
                r"IT\s*(\d+)ìœ„",
                r"ì»´í“¨í„°/ëª¨ë°”ì¼\s*(\d+)ìœ„",
                r"ì»´í“¨í„°\s*(\d+)ìœ„",
            ]

            for pattern in it_patterns:
                match = re.search(pattern, page_text)
                if match:
                    kyobo_data["it_rank"] = int(match.group(1))
                    break

            # prod_rank_area ë˜ëŠ” ìœ ì‚¬í•œ í´ë˜ìŠ¤ì—ì„œ ì°¾ê¸°
            if not kyobo_data["domestic_rank"] or not kyobo_data["it_rank"]:
                rank_areas = [
                    "prod_rank_area",
                    "prod_rank_wrap",
                    "rankArea",
                    "bestRank",
                ]
                for area_class in rank_areas:
                    rank_section = soup.find("div", class_=area_class)
                    if rank_section:
                        section_text = rank_section.get_text()

                        if not kyobo_data["domestic_rank"]:
                            for pattern in domestic_patterns:
                                match = re.search(pattern, section_text)
                                if match:
                                    kyobo_data["domestic_rank"] = int(match.group(1))
                                    break

                        if not kyobo_data["it_rank"]:
                            for pattern in it_patterns:
                                match = re.search(pattern, section_text)
                                if match:
                                    kyobo_data["it_rank"] = int(match.group(1))
                                    break

            # ë©”íƒ€ ë°ì´í„°ë‚˜ JSON-LDì—ì„œ ì°¾ê¸°
            if not kyobo_data["domestic_rank"] or not kyobo_data["it_rank"]:
                script_tags = soup.find_all("script", type="application/ld+json")
                for script in script_tags:
                    try:
                        data = json.loads(script.string)
                        # JSON-LD ë°ì´í„°ì—ì„œ ë­í‚¹ ì •ë³´ ì¶”ì¶œ ì‹œë„
                        if isinstance(data, dict):
                            str_data = str(data)
                            if not kyobo_data["domestic_rank"]:
                                for pattern in domestic_patterns:
                                    match = re.search(pattern, str_data)
                                    if match:
                                        kyobo_data["domestic_rank"] = int(
                                            match.group(1)
                                        )
                                        break

                            if not kyobo_data["it_rank"]:
                                for pattern in it_patterns:
                                    match = re.search(pattern, str_data)
                                    if match:
                                        kyobo_data["it_rank"] = int(match.group(1))
                                        break
                    except:
                        continue

            # dl, dt, dd íƒœê·¸ì—ì„œ ì°¾ê¸°
            if not kyobo_data["domestic_rank"] or not kyobo_data["it_rank"]:
                dl_elements = soup.find_all("dl")
                for dl in dl_elements:
                    dl_text = dl.get_text()
                    if not kyobo_data["domestic_rank"]:
                        for pattern in domestic_patterns:
                            match = re.search(pattern, dl_text)
                            if match:
                                kyobo_data["domestic_rank"] = int(match.group(1))
                                break

                    if not kyobo_data["it_rank"]:
                        for pattern in it_patterns:
                            match = re.search(pattern, dl_text)
                            if match:
                                kyobo_data["it_rank"] = int(match.group(1))
                                break

            logging.info(
                f"êµë³´ë¬¸ê³  ë°ì´í„°: êµ­ë‚´ë„ì„œ {kyobo_data['domestic_rank']}ìœ„, IT {kyobo_data['it_rank']}ìœ„"
            )

        except Exception as e:
            kyobo_data["error"] = str(e)
            logging.error(f"êµë³´ë¬¸ê³  ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        return kyobo_data

    def scrape_yes24(self, url: str) -> Dict[str, Any]:
        """
        YES24ì—ì„œ íŒë§¤ì§€ìˆ˜ì™€ IT ëª¨ë°”ì¼ ìˆœìœ„ ì¶”ì¶œ
        ë©”ì¸ ìƒí’ˆ í˜ì´ì§€ì™€ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ëª¨ë“ˆ í˜ì´ì§€ë¥¼ ëª¨ë‘ í™•ì¸
        """
        logging.info("YES24 ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        yes24_data = {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "sales_index": None,
            "it_mobile_rank": None,
            "error": None,
        }

        try:
            # ë©”ì¸ ìƒí’ˆ í˜ì´ì§€ì—ì„œ íŒë§¤ì§€ìˆ˜ ì¶”ì¶œ
            html = self.fetch_page(url)
            if not html:
                yes24_data["error"] = "ë©”ì¸ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return yes24_data

            soup = BeautifulSoup(html, "html.parser")

            # íŒë§¤ì§€ìˆ˜ ì°¾ê¸°
            sales_patterns = [
                r"íŒë§¤ì§€ìˆ˜\s*[:\s]*(\d+(?:,\d+)*)",
                r"Sales\s*Point\s*[:\s]*(\d+(?:,\d+)*)",
                r"íŒë§¤\s*ì§€ìˆ˜\s*(\d+(?:,\d+)*)",
            ]

            # ì „ì²´ í˜ì´ì§€ì—ì„œ íŒë§¤ì§€ìˆ˜ ê²€ìƒ‰
            page_text = soup.get_text()
            for pattern in sales_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    yes24_data["sales_index"] = match.group(1).replace(",", "")
                    break

            # íŒë§¤ì§€ìˆ˜ê°€ íŠ¹ì • ìš”ì†Œì— ìˆëŠ” ê²½ìš°
            if not yes24_data["sales_index"]:
                # gd_infoBot í´ë˜ìŠ¤ ë‚´ë¶€ ê²€ìƒ‰
                info_section = soup.find("div", class_="gd_infoBot")
                if info_section:
                    text = info_section.get_text()
                    match = re.search(r"íŒë§¤ì§€ìˆ˜\s*(\d+(?:,\d+)*)", text)
                    if match:
                        yes24_data["sales_index"] = match.group(1).replace(",", "")

            # URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œí•˜ì—¬ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ëª¨ë“ˆ URL ìƒì„±
            product_id_match = re.search(r"/goods/(\d+)", url)
            if product_id_match:
                product_id = product_id_match.group(1)
                module_url = f"https://www.yes24.com/Product/addModules/BestSellerRank_Book/{product_id}/?categoryNumber=001001003025009&FreePrice=N"

                logging.info(f"ë² ìŠ¤íŠ¸ì…€ëŸ¬ ëª¨ë“ˆ URL í™•ì¸ ì¤‘: {module_url}")

                # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ëª¨ë“ˆ í˜ì´ì§€ì—ì„œ IT ëª¨ë°”ì¼ ìˆœìœ„ ì¶”ì¶œ
                module_html = self.fetch_page(module_url)
                if module_html:
                    module_soup = BeautifulSoup(module_html, "html.parser")
                    module_text = module_soup.get_text()

                    # IT ëª¨ë°”ì¼ ìˆœìœ„ íŒ¨í„´ - ëª¨ë“ˆ í˜ì´ì§€ì— íŠ¹í™”ëœ íŒ¨í„´
                    it_patterns = [
                        r"IT\s*ëª¨ë°”ì¼\s*(\d+)ìœ„",
                        r"IT\s*ëª¨ë°”ì¼\s*top\d+\s*(\d+)ì£¼",  # "IT ëª¨ë°”ì¼ top20 1ì£¼" í˜•ì‹ë„ ê³ ë ¤
                        r"IT/ëª¨ë°”ì¼\s*(\d+)ìœ„",
                        r"IT\s*/\s*ëª¨ë°”ì¼\s*(\d+)",
                    ]

                    for pattern in it_patterns:
                        match = re.search(pattern, module_text)
                        if match:
                            rank_num = match.group(1)
                            # ìˆœìœ„ê°€ ìˆ«ìì¸ì§€ í™•ì¸ (ì£¼ì°¨ëŠ” ì œì™¸)
                            if (
                                rank_num.isdigit() and int(rank_num) <= 100
                            ):  # 100ìœ„ ì´í•˜ëŠ” ìœ íš¨í•œ ìˆœìœ„ë¡œ ê°„ì£¼
                                yes24_data["it_mobile_rank"] = int(rank_num)
                                break

                    if self.debug and module_text:
                        logging.debug(f"ëª¨ë“ˆ í˜ì´ì§€ ë‚´ìš© ì¼ë¶€: {module_text[:200]}")
                else:
                    logging.warning("ë² ìŠ¤íŠ¸ì…€ëŸ¬ ëª¨ë“ˆ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë©”ì¸ í˜ì´ì§€ì—ì„œë„ IT ëª¨ë°”ì¼ ìˆœìœ„ ì‹œë„ (ë³´ì¡° ìˆ˜ë‹¨)
            if not yes24_data["it_mobile_rank"]:
                it_patterns = [
                    r"IT\s*ëª¨ë°”ì¼\s*(\d+)ìœ„",
                    r"IT/ëª¨ë°”ì¼\s*(\d+)ìœ„",
                    r"IT\s*/\s*ëª¨ë°”ì¼\s*(\d+)",
                    r"ì»´í“¨í„°/IT\s*(\d+)ìœ„",
                    r"ì»´í“¨í„°\s*/\s*ëª¨ë°”ì¼\s*(\d+)ìœ„",
                    r"ì»´í“¨í„°\s*ëª¨ë°”ì¼\s*(\d+)ìœ„",
                    r"IT\s*(\d+)ìœ„",
                    r"IT/ì»´í“¨í„°\s*(\d+)ìœ„",
                ]

                for pattern in it_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        yes24_data["it_mobile_rank"] = int(match.group(1))
                        break

                # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë­í‚¹ ì„¹ì…˜ì—ì„œ ì°¾ê¸°
                if not yes24_data["it_mobile_rank"]:
                    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì‹œë„
                    rank_classes = [
                        "gd_best",
                        "rank_row",
                        "cate_best",
                        "rankRow",
                        "gd_nameH",
                    ]
                    for class_name in rank_classes:
                        rank_elem = soup.find("div", class_=class_name)
                        if rank_elem:
                            rank_text = rank_elem.get_text()
                            for pattern in it_patterns:
                                match = re.search(pattern, rank_text)
                                if match:
                                    yes24_data["it_mobile_rank"] = int(match.group(1))
                                    break
                            if yes24_data["it_mobile_rank"]:
                                break

                # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì •ë³´ê°€ ìˆëŠ” dl, dt, dd íƒœê·¸ ê²€ìƒ‰
                if not yes24_data["it_mobile_rank"]:
                    dl_elements = soup.find_all("dl")
                    for dl in dl_elements:
                        text = dl.get_text()
                        for pattern in it_patterns:
                            match = re.search(pattern, text)
                            if match:
                                yes24_data["it_mobile_rank"] = int(match.group(1))
                                break
                        if yes24_data["it_mobile_rank"]:
                            break

            # ë””ë²„ê¹…: ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ í¬í•¨ëœ ì˜ì—­ ì¶œë ¥
            if not yes24_data["it_mobile_rank"] and self.debug:
                logging.debug(
                    "IT ëª¨ë°”ì¼ ìˆœìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì¹´í…Œê³ ë¦¬ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘..."
                )
                # 'IT', 'ëª¨ë°”ì¼', 'ì»´í“¨í„°' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
                keywords = ["IT", "ëª¨ë°”ì¼", "ì»´í“¨í„°"]
                for keyword in keywords:
                    elements = soup.find_all(text=re.compile(keyword))
                    for elem in elements[:3]:  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
                        if elem and "ìœ„" in elem:
                            logging.debug(f"  ì°¾ì€ í…ìŠ¤íŠ¸: {elem.strip()[:100]}")

            logging.info(
                f"YES24 ë°ì´í„°: íŒë§¤ì§€ìˆ˜ {yes24_data['sales_index']}, ITëª¨ë°”ì¼ {yes24_data['it_mobile_rank']}ìœ„"
            )

        except Exception as e:
            yes24_data["error"] = str(e)
            logging.error(f"YES24 ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        return yes24_data

    def scrape_aladin(self, url: str) -> Dict[str, Any]:
        """
        ì•Œë¼ë”˜ì—ì„œ ìˆœìœ„ ì •ë³´ ì¶”ì¶œ
        - ì»´í“¨í„°/ëª¨ë°”ì¼ ì£¼ê°„ ìˆœìœ„
        - ëŒ€í•™êµì¬/ì „ë¬¸ì„œì  top100 ìˆœìœ„
        - Sales Point
        """
        logging.info("ì•Œë¼ë”˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        aladin_data = {
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "computer_weekly_rank": None,
            "textbook_rank": None,
            "sales_point": None,
            "rank_period": None,
            "error": None,
        }

        try:
            html = self.fetch_page(url)
            if not html:
                aladin_data["error"] = "í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return aladin_data

            soup = BeautifulSoup(html, "html.parser")
            page_text = soup.get_text()

            # ì»´í“¨í„°/ëª¨ë°”ì¼ ì£¼ê°„ ìˆœìœ„
            computer_patterns = [
                r"ì»´í“¨í„°/ëª¨ë°”ì¼\s*ì£¼ê°„\s*(\d+)ìœ„",
                r"ì»´í“¨í„°\s*/\s*ëª¨ë°”ì¼\s*.*?(\d+)ìœ„",
                r"IT/ì»´í“¨í„°.*?ì£¼ê°„\s*(\d+)",
            ]

            for pattern in computer_patterns:
                match = re.search(pattern, page_text)
                if match:
                    aladin_data["computer_weekly_rank"] = int(match.group(1))
                    break

            # ëŒ€í•™êµì¬/ì „ë¬¸ì„œì  ìˆœìœ„
            textbook_patterns = [
                r"ëŒ€í•™êµì¬/ì „ë¬¸ì„œì \s*top\s*100\s*(\d+)",
                r"ëŒ€í•™êµì¬\s*/\s*ì „ë¬¸ì„œì .*?(\d+)ìœ„",
                r"ì „ë¬¸ì„œì .*?top.*?(\d+)",
            ]

            for pattern in textbook_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    aladin_data["textbook_rank"] = int(match.group(1))
                    break

            # ìˆœìœ„ ê¸°ê°„ (ì˜ˆ: 2ì£¼)
            period_match = re.search(r"(\d+)ì£¼\s*\|", page_text)
            if period_match:
                aladin_data["rank_period"] = f"{period_match.group(1)}ì£¼"

            # Sales Point
            sales_patterns = [
                r"Sales\s*Point\s*:\s*(\d+(?:,\d+)*)",
                r"íŒë§¤ì§€ìˆ˜\s*:\s*(\d+(?:,\d+)*)",
                r"Sales\s*Point\s*(\d+(?:,\d+)*)",
            ]

            for pattern in sales_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    aladin_data["sales_point"] = match.group(1).replace(",", "")
                    break

            # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì •ë³´ ì„¹ì…˜ì—ì„œ ì¶”ê°€ ê²€ìƒ‰
            bestseller_section = soup.find("div", class_=re.compile("best|rank"))
            if bestseller_section and not all(
                [
                    aladin_data["computer_weekly_rank"],
                    aladin_data["textbook_rank"],
                    aladin_data["sales_point"],
                ]
            ):
                section_text = bestseller_section.get_text()

                # ì¶”ê°€ íŒ¨í„´ ë§¤ì¹­ ì‹œë„
                if not aladin_data["computer_weekly_rank"]:
                    match = re.search(r"ì»´í“¨í„°.*?(\d+)ìœ„", section_text)
                    if match:
                        aladin_data["computer_weekly_rank"] = int(match.group(1))

                if not aladin_data["sales_point"]:
                    match = re.search(
                        r"(\d+(?:,\d+)*)\s*point", section_text, re.IGNORECASE
                    )
                    if match:
                        aladin_data["sales_point"] = match.group(1).replace(",", "")

            logging.info(
                f"ì•Œë¼ë”˜ ë°ì´í„°: ì»´í“¨í„°/ëª¨ë°”ì¼ {aladin_data['computer_weekly_rank']}ìœ„, "
                f"ëŒ€í•™êµì¬ {aladin_data['textbook_rank']}ìœ„, "
                f"Sales Point {aladin_data['sales_point']}"
            )

        except Exception as e:
            aladin_data["error"] = str(e)
            logging.error(f"ì•Œë¼ë”˜ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

        return aladin_data

    def scrape_all(self, urls: Dict[str, str]) -> Dict[str, Any]:
        """
        ëª¨ë“  ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰

        Args:
            urls: {'kyobobook': url, 'yes24': url, 'aladin': url}

        Returns:
            ëª¨ë“  ìŠ¤í¬ë˜í•‘ ê²°ê³¼
        """
        logging.info(
            f"ëª¨ë“  ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )

        results = {
            "scraping_date": datetime.now().isoformat(),
            "kyobobook": None,
            "yes24": None,
            "aladin": None,
        }

        # êµë³´ë¬¸ê³  ìŠ¤í¬ë˜í•‘
        if "kyobobook" in urls:
            results["kyobobook"] = self.scrape_kyobobook(urls["kyobobook"])
            time.sleep(1)  # ìš”ì²­ ê°„ê²© ë‘ê¸°

        # YES24 ìŠ¤í¬ë˜í•‘
        if "yes24" in urls:
            results["yes24"] = self.scrape_yes24(urls["yes24"])
            time.sleep(1)

        # ì•Œë¼ë”˜ ìŠ¤í¬ë˜í•‘
        if "aladin" in urls:
            results["aladin"] = self.scrape_aladin(urls["aladin"])

        logging.info(
            f"ëª¨ë“  ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )

        return results

    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            filename = f"book_rankings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        logging.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename

    def print_summary(self, results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logging.info("========== ğŸ“š ë„ì„œ ìˆœìœ„ ì •ë³´ ìš”ì•½ ==========")

        if results.get("kyobobook"):
            data = results["kyobobook"]
            if not data.get("error"):
                logging.info(
                    f"ğŸ“˜ êµë³´ë¬¸ê³ : êµ­ë‚´ë„ì„œ {data.get('domestic_rank', 'N/A')}ìœ„, IT {data.get('it_rank', 'N/A')}ìœ„"
                )
            else:
                logging.error(f"ğŸ“˜ êµë³´ë¬¸ê³ : âŒ ì˜¤ë¥˜: {data['error']}")

        if results.get("yes24"):
            data = results["yes24"]
            if not data.get("error"):
                logging.info(
                    f"ğŸ“— YES24: íŒë§¤ì§€ìˆ˜ {data.get('sales_index', 'N/A')}, ITëª¨ë°”ì¼ {data.get('it_mobile_rank', 'N/A')}ìœ„"
                )
            else:
                logging.error(f"ğŸ“— YES24: âŒ ì˜¤ë¥˜: {data['error']}")

        if results.get("aladin"):
            data = results["aladin"]
            if not data.get("error"):
                logging.info(
                    f"ğŸ“™ ì•Œë¼ë”˜: ì»´í“¨í„°/ëª¨ë°”ì¼ {data.get('computer_weekly_rank', 'N/A')}ìœ„, "
                    f"ëŒ€í•™êµì¬ {data.get('textbook_rank', 'N/A')}ìœ„, "
                    f"Sales Point {data.get('sales_point', 'N/A')}"
                )
            else:
                logging.error(f"ğŸ“™ ì•Œë¼ë”˜: âŒ ì˜¤ë¥˜: {data['error']}")
        logging.info("========================================")

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, "client"):
            self.client.close()


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # URL ì„¤ì •
    urls = {
        "kyobobook": "https://product.kyobobook.co.kr/detail/S000217241525",
        "yes24": "https://www.yes24.com/product/goods/150701473",
        "aladin": "https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=369431124",
    }

    # ìŠ¤í¬ë˜í¼ ìƒì„± (ë””ë²„ê¹… ëª¨ë“œëŠ” ì„ íƒì‚¬í•­)
    scraper = BookRankingScraper(debug=False)

    try:
        # ëª¨ë“  ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘
        results = scraper.scrape_all(urls)

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        scraper.print_summary(results)

        # ê²°ê³¼ ì €ì¥
        filename = scraper.save_results(results)

        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥ (ì„ íƒì‚¬í•­)
        logging.debug(f"ìƒì„¸ ê²°ê³¼: {json.dumps(results, ensure_ascii=False, indent=2)}")

    except Exception as e:
        logging.critical(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    finally:
        scraper.close()


# ì„¤ì¹˜ ëª…ë ¹ì–´
# pip install httpx beautifulsoup4

if __name__ == "__main__":
    main()
